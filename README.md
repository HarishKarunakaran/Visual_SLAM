# Visual_SLAM
In this project, we implement Visual SLAM for the case of Unmanned Ground Vehicles in indoor environments. RGB-D sensors (Microsoft Kinect), due to their low-cost and light weight are widely used in cluttered indoor environments. Our algorithm is odometry and vision based thus allowing low cost navigation in such environments. Vision techniques are used to identify and match features in various frames, the motion model is based on the odometry of the robot and an Extended Kalman Filter (EKF) is used for localization and mapping purposes. We show an application of our algorithm on the TUMâ€™s open-source RGB-D SLAM dataset. We have also compared our method with the ground truth trajectories and the state of the art techniques in the field. The results suggest good performance in the case of indoor environments with an average Absolute Trajectory error (ATE) of 22cm per frame.
